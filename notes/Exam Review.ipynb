{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tan ch 1**\n",
    "\n",
    "Objects == Records <br/>\n",
    "Attributes == Features <br/>\n",
    "<br>\n",
    "Types of Data :- \n",
    "- Nominal (no function except identification) Red, Yellow, Blue. Ids. Names.\n",
    "- Ordinal (Descriptive labels that can be ordered) Brightest, Bright, Dark.  \n",
    "- Interval (Labels that have meaningful difference) Temperature. Calendar Days. \n",
    "- Ratio (Numbers that have meaningful distances and ratios) All other numbers\n",
    "\n",
    "Every type includes every types before its characteristics.\n",
    "\n",
    "Categorical is Nominal and Ordinal. Numerical is Interval and Ratio.\n",
    "\n",
    "Asymmetric Attributes :- Attributes where only presence matters.\n",
    "\n",
    "Critiques :- \n",
    "- Asymmetric Binary <br>\n",
    "- Cyclical <br>\n",
    "- Multivariate <br>\n",
    "- Partially ordered <br>\n",
    "- Partial membership <br>\n",
    "- Relationships between the data \n",
    "\n",
    "Real data is approximate and noisy\n",
    "\n",
    "Dataset types :- \n",
    "- Record\n",
    " - Data Matrix\n",
    " - Document Data (Matrix with words as features)\n",
    " - Transaction Data (Matrix with items as features)\n",
    "- Graph\n",
    " - Worldwide Web\n",
    " - Molecular Structure\n",
    "- Ordered\n",
    " - Spatial Data\n",
    " - Temporal Data\n",
    " - Sequential Data\n",
    " - Genetic Sequence\n",
    " \n",
    "\n",
    "Data Quality :- the quality of data determines how good the model will be. As an analyst you cannot control the quality of your data but you can perform pre-processing to make your data cleaner.\n",
    "\n",
    "Data Quality Problems :- \n",
    "- Noise (non relevant data)\n",
    "- Outliers (data that is in the extremes)\n",
    "- Missing Values (blank values)\n",
    "- Duplicate data (attributes or objects that are the same)\n",
    "- Wrong data (erroneous data)\n",
    "\n",
    "Missing Value Types :- \n",
    "- MCAR (missing value is independent)\n",
    " - Missingness of a value is independent of attributes\n",
    " - Fill in values based on the attributes\n",
    " - Analysis may be biased overall\n",
    "- MAR (missing value is used in other variables)\n",
    " - Missingness is related to other variables\n",
    " - Fill in values based other values\n",
    " - Almost always produces a bias in the analysis\n",
    "- MNAR (missing value is related to unknown variable)\n",
    " - Missingness is related to unobserved measurements\n",
    " - Informative and non-ignorable missingness.\n",
    "\n",
    "Similarity - Difference :-\n",
    "- Similarity (Attributes)\n",
    " - Nominal (==) is 1 (!=) is 0\n",
    " - Ordinal $1 - d$\n",
    " - Interval, Ratio s = -d  \n",
    "- Disimilarity (Attributes)\n",
    " - Nominal (==) is 0 (!=) is 1\n",
    " - Ordinal $\\frac{|x - y|}{n-1}$ we do this because we want to establish a distance between variables.\n",
    " - Interval/Ratio |x - y| \n",
    "\n",
    "Distance (Objects) :- \n",
    " - Eucalidian Distance = $ \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2} $\n",
    " - SMC = $\\frac{f_{11} + f_{00}}{f_{11} + f_{10} + f_{01} + f_{00}}$\n",
    " - Jaccard = $\\frac{f_{11}}{f_{11} + f_{10} + f_{01}}$\n",
    " - Cosine = $\\frac{<x \\cdot y>}{|x| * |y|}$\n",
    " - Hamming = $\\sum_{i=1}^{n} (x_i - y_i)$\n",
    " - Minkowski difference = $ (\\sum_{i=1}^{n} |x_i - y_i|^r)^{1/r}$\n",
    "\n",
    "\n",
    "Distances have common properties :-\n",
    " - $d(x,y) >= 0$\n",
    " - Symmetry: $d(x,y) = d(y,x)$\n",
    " - Triangle Inequality: $ d(x,z) <= d(x,y) + d(y,z)$\n",
    " \n",
    "**Covariance** = $$s_{xy} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y}) $$\n",
    "\n",
    "**Standard Deviation** = $$s_x = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n",
    "\n",
    "**Correlation** = $$\\frac{s_{xy}}{s_x*s_y}$$\n",
    "\n",
    "Data Preprocessing :- Removing duplicates, useless data, preparing your data, dealing with missing values.\n",
    "\n",
    " \n",
    "**Entropy** = $$H(x) = - \\sum_{i=1}^{n} p_i\\log_2p_i$$\n",
    "\n",
    "Data Preprocessing :-\n",
    "- Aggregation (combining two attributes to into a single attribute) p(x,y,z) = a\n",
    " - Data Reduction\n",
    " - Change of scale\n",
    " - More \"stable\" data\n",
    "- Sampling (taking a smaller set of the population)\n",
    " - Sampling is as good as population if representative\n",
    " - Simple Random Sampling (select random objects, no way to see if representative)\n",
    " - Stratified Sampling (partition population and then select an amount from each section)\n",
    " - Cross-Validation :- $k$ takes the k$^th$ element in the data set and puts it in the test set. $k-1$ is then the training set data points\n",
    " \n",
    "Dimensionality Curse :- The higher the dimesionality of the dataset the more sparse it becomes. This is because the dimensionality individualizes the objects more. \n",
    "\n",
    "Dimensionality Reduction :- reduces dimesionality while perserving variance\n",
    "- Avoids curse of dimensionality\n",
    "- Save memory and computation time\n",
    "- May help reduce noise and outliers\n",
    "\n",
    "Techniques for dim red :- \n",
    "- Feature selection (selecting \"good\" data)\n",
    "- PCA \n",
    "- Singular Value Decomposition\n",
    "- Suprevised and non-linear dim red\n",
    "- Feature creation (creating a feature our of other features)\n",
    "- Mapping onto new space\n",
    "\n",
    "PCA :-\n",
    "goal - look for a direction that captures the most variance in the dataset.\n",
    "\n",
    "Steps for PCA:\n",
    "- Center data minus all points by the mean\n",
    "- Compute covariance matrix, look at how all data points correlate\n",
    " - Multiplying by the covariance matrix will turn a vector to the direction with the greatest variance in the dataset.\n",
    " - We can use this to find the eigen vector or the vector that the covariance turns to\n",
    "\n",
    "\n",
    "$$ \\sum e = \\lambda e $$\n",
    "**Matrix** ($\\sum$)\n",
    "\n",
    "|dim 1  |dim 2  |\n",
    "|-------|-------|\n",
    "|$x_1$  | $x_2$ |\n",
    "|$x_3$  | $x_4$ |\n",
    "\n",
    "- find eigen values (take covariance matrix $\\sum$)\n",
    " - $det(\\sum - \\lambda I) = 0$\n",
    " - solving the above equation gives us eigen values.\n",
    " - $det(\\sum - \\lambda I) = (x_1 - \\lambda)(x_4 - \\lambda)(x_2)(x_3)= 0$\n",
    " - solve for $\\lambda_1,\\lambda_2$\n",
    " - find eigenvector $\\sum e_i = \\lambda_i e_i$\n",
    "  - $x_1*e_{i,1} + x_2*e_{i,2} = \\lambda_i*e_{i,1}$\n",
    "  - $x_3*e_{i,1} + x_4*e_{i,2} = \\lambda_i*e_{i,2}$\n",
    " - Solving gives you eigen vector for first eigen value.\n",
    " - Continue till you found eigen vector for each eigen value.\n",
    " \n",
    "eigen values = variance along eigen vector.\n",
    "\n",
    "Discretization :- making a continous number discreet by dividing into ranges\n",
    "- Mainly used in classification\n",
    "\n",
    "Binarization :- making a continous number binary by using ranges\n",
    "\n",
    "Attribute transform :- function that maps original to a new set\n",
    "- Normalization is a transform.\n",
    "\n",
    "PC1 maximizes the variance and minmizes **mean squared error**\n",
    "\n",
    "$$ var(A) = \\sum_{i=1}^{n} \\lambda_i$$\n",
    "\n",
    "**error** = $x - \\hat{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zaki ch 2**\n",
    "\n",
    "Mode = most frequent\n",
    "\n",
    "Median = middle of data density function f(m) = 0.5\n",
    "\n",
    "Range = max(x) - min (x)\n",
    "\n",
    "IQR = $F^{-1}(0.75) - F^{-1}(0.25)$\n",
    "\n",
    "Total variance: $var(D) = \\sigma_1^2 + \\sigma_2^2$ \n",
    "\n",
    "Relative variance: $det(D) = \\sigma_1^2 \\sigma_2^2 - \\sigma_{12}^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zaki ch 6**\n",
    "\n",
    "Hyper-rectangle :- \n",
    "\n",
    "**R** = $\\prod_{i=1}^{d} [min(x_i), max(x_i)]$ the min and max specify the range\n",
    "\n",
    "**Hypercube** :-\n",
    "\n",
    "$m = max_{j=1}^{d}max_{i=1}^{n}(|x_{ij}|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tan ch 3**\n",
    "\n",
    "Every dataset has a tuple (x, y) where x is the data and y is the class label\n",
    "\n",
    "We can divided our data into **training set** and **test set**. Each set has the (x, y) tuple.\n",
    "\n",
    "In **training set** we usually sneak in a validation set. So if 80% is training than 20% is validation. **k-fold validation** can be used in this case to make our model better.\n",
    "\n",
    "The reason we have a validation set is because we want to test our model without having it see the test set. We want the test set to be seen the first time to give us more accurate error estimates. The validation set can help determine the best models.\n",
    "\n",
    "**Hyperparameters** :- parameters that are from outside the model like *max_depth*, *max_leaf_size*, *minimum_purity*\n",
    "\n",
    "**Classifier** :-\n",
    "- Decision Trees\n",
    "- Rule-based methods\n",
    "- Nearest-neighbor\n",
    "- Neural Networks\n",
    "- Deep learning\n",
    "- Bayes\n",
    "- Support Vector Machines\n",
    "\n",
    "**Ensemble Classifiers** :- collection of classifiers\n",
    "- Boosting\n",
    "- Bagging\n",
    "- Random forest\n",
    "\n",
    "**You never fit classifiers with input data only the training data determines the shape and model**\n",
    "\n",
    "**Hunt's Alg** :- one of the oldest tree algorithms. If all test data belongs to the same class than it is a leaf node. Otherwise we have to split the node. We just keep splitting the node.\n",
    "\n",
    "**CART** :- A tree algorithm with binary splits which uses the gini index to develop trees.\n",
    "\n",
    "How do we determine test conditions?\n",
    "\n",
    "It depends on the value also depends on how many splits.\n",
    "\n",
    "Tree data type splits :-\n",
    "- Binary - true or false\n",
    "- Nominal/multinominal - a bunch of splits\n",
    "- Ordinal - rank ordering than treat as nominal\n",
    "- Continous - establish threshold and than use binary split\n",
    "\n",
    "**gini**, **entropy**, and **misclassification** are worst at 1, 1/2, 1/2 respectively.\n",
    "\n",
    "Different splits :-\n",
    "- 2-way splits\n",
    "- Multiway splits\n",
    "\n",
    "Discretization :- \n",
    "- Static (you do the discretization in the beginning)\n",
    "- Dynamic (divide at every node)\n",
    "\n",
    "**Greedy Algorithm** :-\n",
    "- The algorithm chooses the purest nodes. \n",
    "\n",
    "**Gini(t)** = $$ 1 - \\sum_{i=1}^{n}p_i^2 $$\n",
    "\n",
    "**Entropy(t)** = $$ - \\sum_{i=1}^{n}p_i\\log_2 p_i $$\n",
    "\n",
    "**Misclassification(t)** = $$ 1 - max(p_i) $$\n",
    "\n",
    "**Gain** = $$i(parent)- i(split)$$\n",
    "\n",
    "**Gain Ratio** = $$ \\frac{Gain}{entropy(t)}$$\n",
    "\n",
    "**Gini split** = $$\\sum_{i=1}^{k}\\frac{n_i}{n}Gini(i)$$\n",
    "\n",
    "**Accuracy** = 1 - error\n",
    "\n",
    "**Error rate** = $$\\frac{FP + FN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Accuracy** = $$\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**Gain Split** = $$ Entropy(parent) - \\sum_{i = 1}^{k} \\frac{n_i}{n} Entropy(i) $$ \n",
    "\n",
    "**Gain Ratio** = $$ \\frac{Gain Split}{-\\sum_{i=1}^{k} \\frac{n_i}{n}\\log_2(\\frac{n_i}{n})} $$\n",
    "\n",
    "**Precision** = $$ \\frac{TP}{TP + FP} $$\n",
    "\n",
    "**Recall** = $$ \\frac {TP}{TP + FN} $$\n",
    "\n",
    "$\\alpha $= FP  know as Type I error <br>\n",
    "$\\beta $= FN  know as Type II error <br>\n",
    "TP is known as **Statistical Power**\n",
    "\n",
    "**Decision Trees** :- \n",
    "- Advantages\n",
    " - Inexpensive to make\n",
    " - fast at classifying\n",
    " - Easy to interpert\n",
    " - robust to noise (Can handle outliers)\n",
    " - can easily handle redundant data\n",
    " - Can put any data as input\n",
    " - Don't need to filter data\n",
    "- Disadvantages\n",
    " - There are many trees to choose from\n",
    " - Doesn't take into account interactions with attributes\n",
    " - Decision boundaries involve single attributes\n",
    " - Redudant splits\n",
    " - Optimal decision tree is np hard.\n",
    " \n",
    "Decision trees are a set of rules comprising a decision path to a leaf.\n",
    "\n",
    "We want to minimize error of DT.\n",
    "\n",
    "Decision trees are high in variance and low in bias.\n",
    "- It is able to fit the data pretty well.\n",
    "\n",
    "Random Forests are high in bias and low in variance\n",
    "\n",
    "In a DT training error goes lower while testing error goes up. \n",
    "\n",
    "Pre-pruning :- stopping tree after it reaches a threshold\n",
    "\n",
    "Post-pruning :- waiting for tree to developed than raise/replace subtree if all leaves in sub tree lead to same conclusion make a leaf.\n",
    "\n",
    "Decision trees are made by looking at a data set and choosing an attribute and split that would give us the most information gain. Once we find that value we split the tree and put all data that fulfills the split condition into one side and all the values that fail the condition on the other side. We do this recursively with each sub tree until we reach the minimum leaf size or our leaves purity reaches a certain threshold.\n",
    "\n",
    "We choose a split by using split entropy, gini or misclassification.\n",
    "\n",
    "$$ \\textbf{Error rate} = \\frac{1}{n} \\sum_{i=1}^n I(y_i \\neq \\hat{y_i})$$ \n",
    "$$ \\textbf{Accuracy} = \\frac{1}{n} \\sum_{i=1}^n I(y_i = \\hat{y_i}) = 1 - \\textbf{Error rate}$$\n",
    "$$ \\textbf{Precision} = \\frac{TP}{TP+FP} $$\n",
    "$$ \\textbf{Coverage} = \\textbf{Recall} = \\textbf{TPR}=\\frac{TP}{TP+FN} $$\n",
    "$$ \\textbf{TNR} = \\frac{\\textbf{TN}}{\\textbf{FP} + \\textbf{TN}} $$\n",
    "$$ \\textbf{FPR} = \\frac{\\textbf{FP}}{\\textbf{FP} + \\textbf{TN}} $$\n",
    "$$ \\textbf{F1-score} = \\frac{1}{\\frac{1}{\\textbf{precision}} + \\frac{1}{\\textbf{recall}}} = \\frac{2 \\cdot \\textbf{precision}\\cdot \\textbf{recall}}{\\textbf{precision} + \\textbf{recall}}$$\n",
    "$$ MSE(mean-square-error) = \\frac{\\sum(y - \\hat{y})^2}{n}$$\n",
    "\n",
    "**Contingency Table** :-\n",
    "\n",
    "|                   |**True Value** |       |\n",
    "|-------------------|---------------|-------|\n",
    "|**Predicted Value**|*True*         |*False*|\n",
    "|*True*             |TP             | FP    |\n",
    "|*False*            |FN             | TN    |\n",
    "\n",
    "**ROC Analysis** :- Uses FPR and TPR to determine how good a classifier is . A good classifier has a high TPR and a low FPR. \n",
    "\n",
    "A model that has TPR == FPR == p is useless we want a model to lean to TPR.\n",
    "\n",
    "**AUC** :- area under the curve. The larger the area the more structure you capture. Not always good to capture structure.\n",
    "\n",
    "**K-fold** :- divides the the dataset **D** into **k** partitions. Each $\\textbf{D}_i$ is treated as the testing set with remaining datasets as training set. We then estimate the $i^{th}$ for every $\\textbf{D}_i$. \n",
    "\n",
    "**Bootstrap** :- we choose $\\textbf{D}_i$ and replace so that the size of $\\textbf{D}_i$ is similar to $\\textbf{D}$ because the probability that a point doesn't exist in $\\textbf{D}_i$ is 37% $\\textbf{D}_i$ has 63% of the data point from $\\textbf{D}$\n",
    "\n",
    "Higher variance classifiers tend to overfit the data. This is called the unstable property of classifiers.\n",
    "\n",
    "**Ensemble Classifiers** :- Allow you to lower variance and bias by combining classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tan ch 5**\n",
    "\n",
    "**Association Rules Mining** :- Given a set of transactions find rules that will predict occurence of items.\n",
    "\n",
    "**item set** :- A collection of one or more items\n",
    "\n",
    "**Support Count** ($\\sigma$) :- amount of transactions that contain itemset\n",
    "\n",
    "**Support** :- ratio of **Support Count** over all transaction count. $ S = \\frac{\\sigma}{n}$\n",
    "\n",
    "**Frequent Itemset** :- any itemset with **Support** that is greater than *minsup*\n",
    "\n",
    "**Association Rule** :- an implication X -> Y where X and Y are **item set**\n",
    "\n",
    "**Confidence** (c) :- support of **item set** of rule {X, Y} over support of left side of rule X\n",
    "\n",
    "**Suprious patterns** :- patterns that don't make sense want to avoid these.\n",
    "\n",
    "Goal of rule mining :- \n",
    "- Find all itemset with support $\\geq$ *minsup* \n",
    "- Find all rules with confidence $\\geq$ *mincon*\n",
    "\n",
    "Brute Force for rule mining looks at all itemsets and prunes those with support less than *minsup* looks at rules and prunes those with confidence less than *mincon*\n",
    "\n",
    "Strategies to generate k$^{th}$ frequent sets :- \n",
    "- Reduce number of candidates\n",
    " - Using pruning to reduce candidates\n",
    "- Reduce number of transactions\n",
    " - Reduce size N as itemset increases in size\n",
    "- Reduce number of comparisons\n",
    " - Use efficient data structure to store candidates and transaction\n",
    " - No need to match every candidate on every transaction.\n",
    " \n",
    "**Apriori Principle** :- \n",
    "\n",
    "If X ⊆ Y, then sup(X) ≥ sup(Y), which leads to the following two observations: \n",
    "- (1) if X is frequent, then any subset Y ⊆ X is also frequent\n",
    "- (2) if X is not frequent, then any superset Y ⊇ X cannot be frequent.\n",
    "\n",
    "The Apriori algorithm utilizes these two properties to significantly improve the brute-force approach. It employs a level-wise or breadth-first exploration of the itemset search space, and prunes all supersets of any infrequent candidate, as no superset of an infrequent itemset can be frequent. It also avoids generating any candidate that has an infrequent subset.In addition to improving the candidate generation step via itemset pruning, the Apriori method also significantly improves the I/O complexity. Instead ofcounting the support for a single itemset, it explores the prefix tree in a breadth-first manner, and computes the support of all the valid candidates ofsize k that comprise level k in the prefix tree\n",
    "\n",
    "Using Apriori we start by 1 itemsets. If a one itemset isn't frequent than no set containing that item will be frequent. Thus you cut off all supersets containing the itemset. As you keep increasing the k items in the itemsets we start prune more and more of the latice till we have frequent itemsets for all k levels.\n",
    "\n",
    "$\\textbf{F}_{k-1}$  x  $\\textbf{F}_1$ strategy :-\n",
    "- Take the $\\textbf{F}_{k-1}$ set and generate new candidate set using $\\textbf{F}_{1}$\n",
    " - Looking at itemsets k without duplicates. Than go and take out any subsets k-2 itemsets that are infrequent\n",
    "- Then we go and count support and isolate the frequent set. \n",
    "\n",
    "$\\textbf{F}_{k-1}$ x $\\textbf{F}_{k-1}$ strategy :-\n",
    "- Arrange all itemsets in $\\textbf{F}_{k-1}$ in lexographic order. \n",
    "- merge pairs of frequent k-1 itemsets if the first k-2 itemsets are the same. \n",
    " - $F_{k-1}$ = {{A,B,C},{A,B,E},{B,C,D}} we would merge {A,B,C} and {A,B,E} to {A,B,C,E}. keeping the lexographic. \n",
    " - {B,C,D} doesn't get merged because if there was support we would have a itemset with {A, B, D} or {B, C, E}\n",
    "- Thus we only generate canidates if the first (k-2) items are identical.\n",
    "\n",
    "Alternate of $\\textbf{F}_{k-1}$ x $\\textbf{F}_{k-1}$ is to merge based on if last (k-2) items and first (k-2) items are similar. Either way we get the same result.\n",
    "\n",
    "In order to reduce comparisons of transaction we use a hash structure. We create buckets and we build a hash structure that allows us to look at each item and put the candidates in specific places.\n",
    "\n",
    "Support count hashes are used to divided transaction to k itemsets. This allows easy enumeration over all k sets in a transaction to count support.\n",
    "\n",
    "canidate sets must be ordered\n",
    "\n",
    "- t = {1, 2, 3, 5, 6}\n",
    "- level 1: 1:{2, 3, 5, 6}  & 2:{3, 5, 6} & 3:{5, 6} # candidates begining with 5 and 6 cannot make 3 itemsets in this t\n",
    "- level 2 (1): 1,2:{3,5 6} & 1,3:{5,6} & 1,5, 6\n",
    "- level 2 (2): 2,3: {5,6} & 2,5,6\n",
    "- level 2 (3): 3,5,6\n",
    ".......\n",
    "\n",
    "Rule Generation (Confidence) :-\n",
    "- Confidence rule\n",
    " - c(ABC -> D) $\\geq$ c(AB -> CD)  $\\geq$ c(A -> BCD) \n",
    " - Thus is c(ABC -> has low confidence than all following rules are also lowconfidence.\n",
    " - if X is the super set of X' and Y is a subset of Y' than c(X -> Y) $\\geq$ c(X' -> Y')\n",
    " \n",
    "Apriori Complexity :-\n",
    "- Choice of *minsup*\n",
    "- Dimensionality\n",
    "- Size of database\n",
    "- Average transaction width.\n",
    "\n",
    "**Maximal Itemset** :- \n",
    "- if I is frequent and all super sets are infrequent.\n",
    "\n",
    "**Closed Itemset** :- \n",
    "- if none of I's supersets have same support than I.\n",
    "\n",
    "**Contigency Table** :-\n",
    "\n",
    "|         |$Y$     |$\\bar{Y}$|        |\n",
    "|---------|--------|---------|        |\n",
    "|$X$      |$f_{11}$|$f_{10}$ |$f_{1+}$|\n",
    "|$\\bar{X}$|$f_{01}$|$f_{00}$ |$f_{0+}$|\n",
    "|         |$f_{+1}$|$f_{+0}$ |N       |\n",
    "\n",
    "$\\textbf{f}_{11}$ = support($X \\cup Y$) <br>\n",
    "$\\textbf{f}_{10}$ = support($X \\cup \\bar{Y}$) <br>\n",
    "$\\textbf{f}_{01}$ = support($\\bar{X} \\cup Y$) <br>\n",
    "$\\textbf{f}_{00}$ = support($\\bar{X} \\cup \\bar{Y}$) <br>\n",
    "\n",
    "To make sure we have **reliable** rules we use  con(x -> y) $\\geq$ sup(y)\n",
    "\n",
    "**independence** is con(x -> y) == sup(y)\n",
    "\n",
    "**Lift** = $\\frac{\\textbf{c}(x -> y)}{\\textbf{s}(y)} = \\frac{\\frac{\\textbf{s}(x,y)}{\\textbf{s}(x)}}{\\textbf{s}(y)} = \\frac{\\textbf{s}(x,y)}{\\textbf{s}(x)\\textbf{s}(y)}$ \n",
    "\n",
    "**Lift** is used for rules and thus is $$\\frac{\\textbf{c}(x -> y)}{s(y)}$$\n",
    "\n",
    "**interest** is used for itemset and thus is $$\\frac{\\textbf{s}(x,y)}{\\textbf{s}(x)\\textbf{s}(y)}$$\n",
    "\n",
    "Another formula for **interest** is $conf(I -> j) - s(j)$\n",
    "\n",
    "if **Lift** is less than 1 than the itemsets are negatively associated.\n",
    "\n",
    "if **Lift** is 1 than the itemsets are statistically independent.\n",
    "\n",
    "If **Lift** is greater than 1 than it positively associated. \n",
    "\n",
    "**Pearson Corr** = $$ \\frac{f_{11}f_{00} - f_{10}f_{01}}{\\sqrt{f_{1+}f_{0+}f_{+0}f_{+1}}}$$\n",
    "\n",
    "**LS** (used in asymmetric datasets) = $$\\sqrt{c(a -> b) \\cdot c(b -> a)}$$\n",
    "\n",
    "**LS** provides a way to compare correlation of different object like **Lift**\n",
    "\n",
    "**LS** is compared to the cosine similarity.\n",
    "\n",
    "**Regular Pearson Corr** = $$ \\frac{Cov(X,Y)}{\\sigma{X}\\sigma{Y}} = \\frac{Cov(X,Y)}{\\sqrt{Var(X)}\\sqrt{Var(Y)}}$$\n",
    "\n",
    "Hash trees hash the candidate keys which makes it easier for transactions to count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
