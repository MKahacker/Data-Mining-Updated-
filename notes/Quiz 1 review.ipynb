{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review for quiz\n",
    "\n",
    "data mining is concerned with extracting valid, useful, unexpected,and understandable patterns.\n",
    "Descriptive models find human interpertable patterns.\n",
    "Predictive models attempt to predict the future.\n",
    "Why data mining?\n",
    "there is a bunch of data collected,and computers have become cheaper.\n",
    "\n",
    "Data mining has three steps preprocessing,data mining,and posprocessing.\n",
    "Motivating  \n",
    "- Challenges\n",
    "- Scalability\n",
    "- High  Dimensionality\n",
    "- Heterogeneous  and  Complex  Data\n",
    "- Data  Ownership  and  Distribution\n",
    "- Non-traditional  Analysis\n",
    "\n",
    "**attribute** a property or characteristic or feature of an object.\n",
    "\n",
    "Measurement is really important when giving values to attributes because it determines order and scale.\n",
    "\n",
    "There  are  different  types  of  attributes\n",
    "- Nominal Examples:  ID  numbers,  eye  color,  zip  codes\n",
    "- Ordinal Examples:  rankings  (e.g.,  taste  of  potato  chips  on  a  scale  from  1-10),  grades,  height  {tall,  medium,  short}\n",
    "- Interval Examples:  calendar  dates,  temperatures  in  Celsius  or  Fahrenheit.\n",
    "\n",
    "- Ratio Examples:  temperature  in  Kelvin,  length,  time,  counts \n",
    "\n",
    "For  ratio  variables,  both  differences  and  ratios  are  meaningful.  (*,  /)\n",
    "\n",
    "**Asymmetric  Attributes** when the presence of a 0 value is unimportant. Used for transaction analysis.\n",
    "\n",
    "Real  data  is  approximate  and  noisy\n",
    "\n",
    "- Dimensionality  (number   of  attributes) High  dimensional   data  brings  a  number  of  challenges\n",
    "- Sparsity Only  presence  counts\n",
    "- Resolution Patterns  depend   on  the   scale  \n",
    "- Size Type  of  analysis  may  depend  on  size  of  data\n",
    "\n",
    "data  quality  problems:  \n",
    "- Noise  and  outliers  \n",
    "- Missing  values  \n",
    "- Duplicate  data  \n",
    "- Wrong  data\n",
    "\n",
    "Similarity and dissimilarity for attributes is calculated using\n",
    "!= and == for nominal d= 1 is != and s = 0\n",
    "d = abs(x - y) / (n-1) for ordinal where s = 1 - d\n",
    "d = abs(x - y) for interval and ration s = -d\n",
    "\n",
    "Euclidean distance is used for objects (should standerize if scales are different)\n",
    "sqrt(sum_from_1_to_n((x_k - y_k)^2))\n",
    "\n",
    "Minkowski difference is generalization of Euclidean\n",
    "(sum_from_1_to_n((x_k - y_k)^r))^(1/r) basically encomposis most distances.\n",
    "\n",
    "r = 1 is manhattan distance\n",
    "r = 2 is Euclidean\n",
    "r = inf is supremem\n",
    "\n",
    "Binary similarity\n",
    "Simple  Matching  and  JaccardCoefficients  \n",
    "SMC  =    number  of  matches  /  number  of  attributes  =    (f11+ f00) / (f01+ f10+ f11+ f00)\n",
    "\n",
    "J  =  number  of  11  matches  /  number  of  non-zero  attributes=  (f11) / (f01+ f10+ f11) \n",
    "\n",
    "\n",
    "cos(d1,d2)=<d1,d2>/||d1||||d2||\n",
    "looks at angles. Does a dot product of d1 and d2 and then divides by unit vectors.\n",
    "\n",
    "correlation is covariance(x, y)/ std(x) * std(y)\n",
    "\n",
    "covariance = (1/(n-1))* sum_from_1_to_n ((x_k - avg(x)) * (y_k - avg(y)))\n",
    "\n",
    "std = sqrt(sum_from_1_to_n((x_k - avg(x))^2))\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "$H(x) = - sum_from_1_to_n(p_i log_2 p_i)$\n",
    "\n",
    "entropy meausres how many bits\n",
    "\n",
    "Types of sampling\n",
    "\n",
    "Simple random sampling: randomly choose objects to sample\n",
    "\n",
    "Stratified sampling: divided data into partitions and then randomly sample.\n",
    "\n",
    "Dimensionality reduction: reduces dimensionality preserves variance.\n",
    "\n",
    "PCA \n",
    "- goal to find projection with highest captured variance\n",
    "\n",
    "Redudant features, irrelevant features are two other ways to reduce dimensionality.\n",
    "\n",
    "You can also create new features that capture variance.\n",
    "\n",
    "**Discretization** turns continous to discreet\n",
    "**Binarization** turns continous to binary.\n",
    "\n",
    "probability density function P(x in (a,b)) = integral_from_a_to_b(f(x)dx)\n",
    "\n",
    "r = max(x) - min(x)\n",
    "\n",
    "z = (x_i - avg(x))/var(x)\n",
    "\n",
    "error = x - x_hat\n",
    "\n",
    "PCA 1 is the one that captures the most variance and lowers Mean Squared Error.\n",
    "\n",
    "When doing classification modeling we usually divided data between:\n",
    "\n",
    "Training set\n",
    "\n",
    "Test set\n",
    "\n",
    "**Hunt's Alg**: if all test data belongs to the same class than it is a leaf node otherwise if there are two or more classes split using a attribute test so that it splits into subset recursively apply to test data. \n",
    "\n",
    "A split in a tree can be multiway (multiple splits) or binary \n",
    "\n",
    "Continous attributes can be split using discretization and binary decisions. The first uses ranges to split mutliway while a binary is achieved through a threshold.\n",
    "\n",
    "**Greedy Approach** to splitting is to choose the purest distributions for splits.\n",
    "\n",
    "GINI(t) = 1 - sum_to_1_from_n((p_j)^2)\n",
    "Entropy(t) = - sum_to_1_from_n(p_j*(log_2(p_j)))\n",
    "error(t) = 1 - max(p_i)\n",
    "\n",
    "P is impurity before splitting\n",
    "M is impurity after splitting\n",
    "\n",
    "Gain = P - M.\n",
    "\n",
    "GainRatio = Gain/Entropy(t)\n",
    "\n",
    "Accuracy = 1 - Error(t)\n",
    "\n",
    "Error_rate = (FP + FN)/ n = (FP + FN) / (TP + TN + FP + FN)\n",
    "\n",
    "Accuracy = (TP + TN)/n = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP) out all the attributes that are identified as true what ratio is actually true\n",
    "\n",
    "recall = TP / (TP + FN) out all true objects how many were identified.\n",
    "\n",
    "Alpha is FP \n",
    "Beta is FN\n",
    "\n",
    "\n",
    "|  | P | N |\n",
    "| --- | --- | --- |\n",
    "| P | TP | FP |\n",
    "| N | FN  | TN |\n",
    "\n",
    "K fold cross validation:\n",
    "\n",
    "Divides dataset D to K equal-sized parts called folds namely, D1, D2, D3...DK.\n",
    "\n",
    "Each Di is than treated as the test set with the rest of the data being the training. After training our model with D - Di and testing it with Di to obtain the ith-estimate.\n",
    "\n",
    "We can than computer the expected(average) value of the Dis and the variance.\n",
    "\n",
    "## Association Analysis\n",
    "\n",
    "Used manily on record data specifically transaction data.\n",
    "\n",
    "Association rules define relationships between two item sets.\n",
    "\n",
    "Itemsets are groups of items.\n",
    "\n",
    "Support count is the number of times the item set occurs in overall.\n",
    "\n",
    "Support is support count over overall count\n",
    "\n",
    "Frequent itemset is any itemset with support over *minsup*\n",
    "\n",
    "**Frequent Itemset Generation** find all itemset with S > minsup (Usually computational expensive)\n",
    "\n",
    "**Rule Generation** find all rules given frequent item set with c > minconf\n",
    "\n",
    "The two main metrics in association are support and confidence. \n",
    "\n",
    "Confidence is number of y as a ratio of x where x is a subset of y.\n",
    "\n",
    "Association Rule Mining\n",
    "finds all itemsets with support >minsup\n",
    "and all rules with confidence >minconf\n",
    "\n",
    "Steps\n",
    "- Generate frequent itemset support > minsup\n",
    "- Looks for rules in frequent itemset confidence > minconf\n",
    "\n",
    "When generating association rules you should reduce members by reducing M = 2^d.\n",
    "You should reduce number of transactions.\n",
    "You should reduce number of comparisons.\n",
    "\n",
    "Apriori Principle if an itemset is frequent than all of it subsets are frequent. This is because any subset of a frequest itemset Y will have an equal amount or more support. \n",
    "\n",
    "Using Apriori we start by 1 itemsets. If a one itemset isn't frequent than no set containing that item will be frequent. Thus you cut off those items in the next round of 2 itemsets. As you keep increase the k items in the itemsets we get more and more specific about what itemsets will make our setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
