{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Used as a tool to look at natural grouping in data.\n",
    "\n",
    "**Unsupervised**: No labels\n",
    "\n",
    "**Uses**:-\n",
    "- Summarization\n",
    "- Compression\n",
    "- Nearest Neighbor\n",
    "- Preprocessing to look for natural groupings to help with model efficiency.\n",
    "\n",
    "Outline of topics\n",
    "\n",
    "-------NOT DONE-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First half of lecture notes in sketch notebook. Should transcribe here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Partitional, prototype based clustering -> 1 -level partitioning\n",
    "- Prototype: Centroid or Mediod\n",
    "- Metric: Mean or Median\n",
    "\n",
    "Distance measure for mean and proximity measure for median.\n",
    "\n",
    "Can work in d-dimensional space but d-dimensional prototype metric has to be used.\n",
    "\n",
    "How it works :- \n",
    "Step 0: Select k\n",
    "Step 1: Assign random $c_i$ to each point. Range of c = $c_i ... c_k$\n",
    "Step 2: update/calculate cetroid (mean/median) for each $c_i$\n",
    "Step 3: Assign each point to its closest cetriod $c_i$\n",
    "\n",
    "We repeat 2 and 3 until \"measure of improvement\" < epsilon. Meaning the we don't improve much by redoing 2 and 3.\n",
    "We terminate when we don't improve anymore.\n",
    "\n",
    "Example $c_7$ = {$x_1$, $x_2$, $x_3$,$x_4$} for d = 4\n",
    "These are the cetriods of $c_7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation :-\n",
    "- X is Object/record\n",
    "- $C_i$ $i^{th}$ cluster\n",
    "- $c_i$ centroid of cluster $C_i$\n",
    "- $m_i$ number of objects in cluster\n",
    "- $K$ number of clusters\n",
    "\n",
    "\"Assigning points to closets $C_i$\"\n",
    "- We need distance measure between X and $C_i$, d(X, $C_i$)\n",
    "    - We can use $L_2$\n",
    "    - Jaccard (Documents)\n",
    "    - Cos (Documents)\n",
    "\n",
    "Steps 1 and 3 have some sort of objective function from a optimization stand point that :-\n",
    "- SSE = $\\sum_{i=1}^{k}\\sum_{X\\in C_i} d(c_i, X)^2$\n",
    "\n",
    "If we minmize SSE than \n",
    "$$c_i = \\frac{1}{m_i} \\sum_{X\\in C_i} X = \\mu_{c_i}$$\n",
    "\n",
    "We need the overall SSE to go down every time the we iterate over 2 and 3. We can do this by improving SSE for each cluster and that way the overall SSE goes down.\n",
    "\n",
    "Every time we recalculate the centroid the SSE changes. \n",
    "\n",
    "When we minimize $c_i$ and we use $L_2$ norm than we result in $c_i$ being the mean.\n",
    "\n",
    "SSE is then an objective function that needs to be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is a :- \n",
    "- greedy alg\n",
    "- make local optimization.\n",
    "\n",
    "This is because most of the space is empty and the algorithm looks for local groups and optimizes on those local groupings.\n",
    "\n",
    "What is the right k to use :-\n",
    "- For each $C_i$ we get the average distance to the cetroid. Then take an average across $c_i$s than we plot against k.\n",
    "    - The average tells us how cohisive our clusters are, how packed they are. Are the points close to each other in the cluster.\n",
    "    - The other thing we want is that the clusters are far from each other.\n",
    "    \n",
    "There are small distances inside the cluster and there are large distances outside the clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with k-means \n",
    "\n",
    "Local optima :- \n",
    "- Initial assignments (We can choose to do N bootstraps and then do N clusterings)\n",
    "- Greedy local choice (for this case there is not much we can do)\n",
    "\n",
    "Using the bootstrap to do soo many clusterings and then pick the best clusterings with the best intial assignments.\n",
    "\n",
    "Empty Clusters :- \n",
    "- When assigning points a cluster doesn't have points\n",
    "    - Solve by splitting the cluster with the worst SSE to get back to K clusters.\n",
    "    \n",
    "Outliers :- \n",
    "- Push centroids to weird places when included to a cluster.\n",
    "    - Can remove outliers, filter outliers, and other strategies.\n",
    "- Leaving points in there to force algorithm to model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Process \n",
    "\n",
    "Increase k :- \n",
    "- Split Cluster (SSE goes down because two clusters have less distance between points)\n",
    "\n",
    "Decrease k :- \n",
    "- Merge Cluster (SSE goes up because distance becomes more spread)\n",
    "    - We usually merge with closest cluster to offending cluster\n",
    "    - We merge if a cluster has a small representation compared to other clusters.\n",
    "    \n",
    "K-means is stupid and you should look at the data to see if clustering of that makes sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bisceting K-Means\n",
    "\n",
    "Divise Hierarchial Clustering :- (Final partional result)\n",
    "- We start with a higher cluster by using a smaller k and then slicing to our desired k. \n",
    "\n",
    "Algorithm :- \n",
    "Step 0: List of K =1 cluster with all points assigned to it\n",
    "Step 1: Pop cluster from list with worst SSE\n",
    "Step 2: Try mutiple bisections of cluster with k=2 runs k-means algoritm with k = 2.\n",
    "Step 3: Select two clusters from results of k-means k=2 with lowest SSE.\n",
    "Step 4: Push both clusters to our heap and increment k.\n",
    "We repeat 1 - 4 till k is our desired k.\n",
    "\n",
    "We Repeat 1 and 2 \n",
    "\n",
    "The pros of this approach is we don't worry about the initial assignment of the vanilla k-means alg.\n",
    "\n",
    "Partitional/prototype clustering :- \n",
    "- clusers tend to be spherical/globilar\n",
    "- Uniform in density/sparisty.\n",
    "\n",
    "Lloyds algorith <-> Kernel k-means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
