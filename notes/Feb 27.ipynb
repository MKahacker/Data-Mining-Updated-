{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adminstration\n",
    "\n",
    "Quiz are gonna be online. Quizzes are gonna be specific to LHS and Links. The first after the midterm and the second sometime after. \n",
    "\n",
    "\n",
    "Exam is not gonna have code or mathematical proofs. Exams will be intuative. Ask general questions about working with data. Ask about covariance matrix.  \n",
    "\n",
    "Make sure you do the reading from the Tan book. Do the homework problems and fiddle with data you'll be fine on the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Principle\n",
    "\n",
    "if an itemset is frequent, so are its subsets\n",
    "\n",
    "if an itemset is infrequent, so are its supersets.\n",
    "\n",
    "Let J be a powerset. where $$J = 2^I$$. I is the itemset.\n",
    "Monotone is x,y are in J and x is a subset of y then if $$f(x) <= f(y)$$ (monotone) \"Upward Close\"\n",
    "\n",
    "Anti-Monotone is x,y are in J and x is a subset of y then if $$f(y) <= f(x)$$ (anti-monotone) \"Downward Close\"\n",
    "\n",
    "Using this background we come up with the apriori algorithm.\n",
    "\n",
    "It looks at all 1s sets eliminates infrequent, and goes to 2s sets that are only with the frequent items. It then eliminates the 2s sets that are infrequent and go to the 3s set. We continue like this till we reach a level that doesn't meet our *minsup* and that is our frequent itemset space. Terminate when you reach a level with an empty set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori Principle (Candidate Gen)\n",
    "\n",
    "**Brute force** at each level k where k is the length of set. $C_k = d$ choose k\n",
    "\n",
    "We use hash trees for each level k.  $$h(x[k]) = x[k]*mod(i)$$ this is max leaf size.\n",
    "\n",
    "Hash trees store the counts off all the k itemsets.\n",
    "\n",
    "We use a **Prefix Ordering**.\n",
    " - Lexicographic ordering\n",
    "\n",
    "($F_{k-1} * F_1$):- if $F_{k-1}$ and $F_1$ is frequent than we do a cartesian product. $F_k$ in this case is all the frequent itemsets at a level. \n",
    "- Redudance \n",
    "\n",
    "$F_{k-1} * F_{k-1}$ :- Find a common $F_{k-2}$ subset and then merge with each $F_1$. \n",
    "\n",
    "$F_1$ is the 1s set of frequent itemsets.\n",
    "- You look at common item sets and use that to merge with that to the next k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of Apriori Principle\n",
    "- Lower minsup means more computation\n",
    "- Dimensions: D(items), n(Transactions), |w|(average transaction width how many items in transaction)\n",
    "\n",
    "$F_1$: $O(|w|x n)$ <br/>\n",
    "$C_k$: every k-1 itemset is frequent $$O(\\sum_{2}^{|w|}(k-2)*|F_k-1|^2)$$\n",
    "Hash Tree: $$O( \\sum_{2}^{|w|}k*|C_k|)$$ to depth k\n",
    "\n",
    "large depths are super slow and if k approaches d than it is very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Gen\n",
    "\n",
    "We have ${{f_1}, {f_2},.....}$\n",
    "\n",
    "Each frequent k itemset -> $2^k -2$ Association Rules.\n",
    "\n",
    "*minconf* for confidence but this a ratio. Thus we cannot use Apriori Principle. \n",
    "\n",
    "if confident of(x -> (y-x)) < *minconf*\n",
    "\n",
    "Then (x' -> (y - x')) where x' subset x and x' subset of y than we can know that confidence of x' is less than *miconf*\n",
    "\n",
    "Thus if a superset doesn't meet confidence than its subset rules don't meet confidence.\n",
    "\n",
    "$h_1$ = {i | i in $f_k$}\n",
    "\n",
    "Create $f_k$ -> $h_1$\n",
    "- k = $|f_k|$\n",
    "- m = $h_m$\n",
    "- enumerate items of $h_m$\n",
    "\n",
    "check confidence\n",
    "\n",
    "\n",
    "Basically the algorithm takes the biggest $f_k$ and then it tries to disprove by putting most sets element on the right side and one element on the left if the confidence is low than we know any superset with the choosen right won't work. By doing that it can disprove and also disprove subset of the set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed Representation\n",
    "\n",
    "- Lots of $f_ks$\n",
    "    - support count (need this for confidence and rule gen)\n",
    "\n",
    "We need to summarize -> compact item set.\n",
    "\n",
    "If we can get set of frequent item sets \"at the border\", meaning I want a item set that is frequent but all supersets aren't. These item sets can be marked with a maximumaly frequent marker that tells us that no other super set was created. We aren't interested in keeping infrequent items and thus completely erase them from our lattice.\n",
    "\n",
    "Support count compression\n",
    "\n",
    "Most of the time alot of items have the same support count.\n",
    "- Closed Frequent itemset: None of its immediate supersets have the same support\n",
    "- if one of X's supersets has the same support -> X is not closed. \n",
    "\n",
    "I = {$i_1$, $i_2$}\n",
    "- every transaction containing I, also includes {$i_3$}, I = {$i_1$, $i_2$, $i_3$} will the same support. \n",
    "\n",
    "If X is closed than I know that all subsets have the same support counts and I can compress based on closed sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All itemsets = {Closed Frequent sets, Closed infrequent sets, frequent open}\n",
    "Closed Frequent sets = {max frequent}\n",
    "\n",
    "Max frequent = itemsets that define the border where the supersets are infrequent and thus the itemsets are closed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Measurements for **Interesting** and **Reliable**\n",
    "\n",
    "**interesting** is when you see unexpected patterns in your association rules\n",
    "\n",
    "**Reliable** is when something happens frequently\n",
    "\n",
    "we have **domain** and **statistical**\n",
    "\n",
    "We want actionable association rules. \n",
    "\n",
    "Alot of noise in transaction database.\n",
    "\n",
    "You would ideally want to achieve high **domain** and high **statistica**.\n",
    "\n",
    "**Contigency table** frequency of binary vars\n",
    "\n",
    "| | B | !B |\n",
    "| --- | --- | --- | \n",
    "| A | f11 | f10 | f1+\n",
    "| !A | f01 | f00 | f0+\n",
    "| | f+1 | f+0 | N\n",
    "\n",
    "\n",
    "A contigency table would tell us if we have a relationship between A and B. This will be apperent in f11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\n",
    "\n",
    "| | Coffee | !Coffee |\n",
    "| --- | --- | --- | \n",
    "| Tea | 150 | 50 | 200\n",
    "| !Tea | 650 | 150 | 800\n",
    "| | 800 | 200 | 1000\n",
    "\n",
    "{tea} -> {coffee}\n",
    "\n",
    "s(tea union cofee) = 150/1000 = 15%\n",
    "\n",
    "c(tea -> coffee) = 150/200 = 75%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support lower support -> spurious cross-support patterns (alot rules that might not be reliable)\n",
    "\n",
    "Cofidence -> ignore support of consequent. You only look at A but you don't look at support for B.\n",
    "\n",
    "This is where Nomalize/scale by consequent support\n",
    "- Lift: $$\\frac{C(A->B)}{S(B)}$$ so $$\\frac{S(A \\cup B)}{S(A)*S(B)}$$. \n",
    "    - Interest Factor $$I(A,B) = \\frac{S(A,B)}{S(A)*S(B)} = \\frac{N*f_{11}}{f_{1+}f_{+1}}$$ if I = 1 -> independent.\n",
    "    \n",
    "$I(A,B)$ -> if 1 > then negative corr and 1 < is positive corr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our example\n",
    "\n",
    "I(coffee, tea) = (1000)x(150)/(200 x 800) = 0.9375 which implies a negative correlation. \n",
    "\n",
    "Thus tea and coffee aren't actually related.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Corr\n",
    "\n",
    "$$ binary = \\frac {f_{11} f_{00} - f_{01} f_{10}}{\\sqrt{f_{1+} f_{+1} f_{0+} f_{+0}}}$$\n",
    "\n",
    "Not invariant undernull addition because it looks at 0 cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lift (scaling)\n",
    "\n",
    "(Scaling Lof Lift (interest factor) by support)\n",
    "\n",
    "$$Is(A,B) = \\sqrt{I(A,B)*S(A,B)} = \\frac{S(A,B)}{\\sqrt{S(A)*S(B)}} = \\sqrt{c(a->b)*c(b->a)}$$ \n",
    "- Using this we can look at the support for both the inverse and other way of the relationship. \n",
    "\n",
    "We treat the support of an itemset as magnitude for binary vars.\n",
    "\n",
    "$$Is(A,B) = \\frac{A}{|A|} * \\frac{B}{|B|}$$\n",
    "\n",
    "this is a cosine similarity and thus if we have a small angle and both items are pointing in the same direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
