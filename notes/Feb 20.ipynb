{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feb 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Evaluation\n",
    "\n",
    "**Regression** \n",
    "On order to test model we divided into a **Training** and **Test** set.\n",
    "\n",
    "error of **Test** prediction in MSE = $$ \\frac{\\sum (y - y_p)^2}{n} $$\n",
    "MSE can be calculated for both train and test although test is more reflective of realworld error.\n",
    "\n",
    "Another test is to check if y and y_p agree than we have 0\n",
    "if y and y_p don't agree than we have 1\n",
    "we then sum and average. \n",
    "\n",
    "**Classification** uses cross validation for tests. using various test sets. \n",
    "\n",
    "**Classifiers** become more fitted to the dataset. The more parameter/complexity you add to the training the less error for Training **MSE**\n",
    "Test is the completely opposite where the more complexity the more **MSE** for the test.\n",
    "\n",
    "There is a sweet spot (Perfect model) where both Test **MSE** and Training **MSE** are minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "I have a data set D it has members {yes, no}. \n",
    "\n",
    "**Class Imbalance** every N dataset has a majority of *noise* (things we don't care about) and then we have *class of interest*. \n",
    "\n",
    "That *class of interest* is what we want to zoom in on.\n",
    "\n",
    "If we have the D set (mentioned above) most data is no. But we want to look at the yes set. \n",
    "\n",
    "- True Positive: y = t and y_p = t\n",
    "- True Negative: y = n and y_p = n\n",
    "- False Positive: y = n and y_p = t\n",
    "- False Negative: y = t and y_p = n\n",
    "\n",
    "**Accuracy** = $$ \\frac{TP + TN} {TP + TN + FP + FN}$$ what I got right over total.<br/>\n",
    "**Error rate** = $$\\frac{FP + FN} {TP + TN + FP + FN}$$ what I got wrong over total\n",
    "\n",
    "this is where we use the **Confusion Matrix** to see what we predicted right and what we predicted wrong.\n",
    "\n",
    "**Statstical Power** a True Positive.\n",
    "\n",
    "Accurate models can seperate classes\n",
    "\n",
    "A True positive model is a very strong model.\n",
    "\n",
    "A **True Negative model** can be very accurate but doesn't really do what we want. \n",
    "\n",
    "Predicting a negative class is necessary but not the point.\n",
    "\n",
    "A **False Positive** is a type 1 error (alpha)\n",
    "\n",
    "A **False Negative** is a type 2 error (Beta)\n",
    "\n",
    "As an analyst should look at positive for non error and negative for error situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**  = $$ \\frac{TP} {TP + FP} $$ 0 if no precision and 1 if alway precise.\n",
    "Measures if we mark everything as positive. If we do than we aren't precise. Out of everything we identified as correct how many were correct.\n",
    "\n",
    "**Recall** = $$\\frac{TP} {TP + FN} $$ Sensitivity\n",
    "Doesn't get enough of the positive data. Model is unable to find positive effectively.\n",
    "Marks alot of things as negative. Out of all correct how many did we identify\n",
    "\n",
    "**Precision** and **Recall** are opposites and if you make one better you probably lose another. \n",
    "\n",
    "**F1 score** = $$ \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} =  \\frac{2*recall*precision}{recall + precision}$$\n",
    "F1 score takes the worst and uses it as the average.\n",
    "\n",
    "**Cost** \n",
    "The cost of FP (False positive) and FN (False negative) aren't the same.\n",
    "\n",
    "All the metrics above treat FP and FN as equal.\n",
    "\n",
    "You have to determine wheather FP or FN is important. \n",
    "\n",
    "In safety for example FN is very important and should be avoided.\n",
    "\n",
    "While in cancer patient analysis it is important to avoid FP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR** = $$ \\frac{TP}{TP + FN} $$\n",
    "**FPR** = $$ \\frac {FP}{TN + FP} $$\n",
    "\n",
    "**TPR** and **FPR** relationship<br/>\n",
    "M1 is **TPR** = 1 and **FPR** = 0. Meaning it we are soo good at predicting the class, that we don't predict any FN or FP <br/>\n",
    "M2 is **TPR** = 1 and **FPR** = 1. Means that we are just predicting everything as positive. <br/>\n",
    "M2's vector connection is called **Random Prediction** any point that lies on that vector is random.\n",
    "In M2's case **TPR** and **FPR** is equal to *P*. meaning that we have a useless model. \n",
    "We want to create a model that leans more towards **TPR**.\n",
    "\n",
    "We want to pick a model that has a **FPR** we are comfortable with usually while gaining the most **TPR** rate.\n",
    "\n",
    "You can change decision boundaries in your classifier to manipulate your **TPR** and **FPR**. By manipulating the decision thresholds for models we can make it better or worse.\n",
    "\n",
    "We should know how to choose a good model. \n",
    "\n",
    "**AUC** (area under the curve) for a model, a greater **AUC** means you are capturing the most structure in the features but it doesn't mean you should pick it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Analysis\n",
    "\n",
    "**Application** \n",
    "- Business => Logistics/Operations\n",
    "    -  Transactions/Inventory (Databases for transactions and inventory)\n",
    "        -  Mine databases to find out what is in season? who buys most? where to put them?\n",
    "        -  Analyzing transaction histories\n",
    "        -  \"Market Basket\" (groups of items bought together)\n",
    "\n",
    "Whats in a transaction record?\n",
    "- Txn Id\n",
    "- Cust Id -> Cust metadata\n",
    "- store Id -> company metadata\n",
    "- Timestamp -> Promos,coupons. \n",
    "- Items -> Item Metadata (What to recommend you)\n",
    "\n",
    "Why data mine company information?\n",
    "- Learn behavior and patterns (Customer Behavior)\n",
    "    - Make more money or save money\n",
    "    - Marketing folks love data mining\n",
    "        - Try to find rich customers\n",
    "        - Try to find people who buy the most\n",
    "        - Try to find people who return the most\n",
    "        - Analyze your target market and make it more beneficial\n",
    "    - Advertising\n",
    "        - Finding people who buy globally\n",
    "        - Look at which markets are buying\n",
    "    - CRM \n",
    "        - Finding lost customers\n",
    "            - Trying to get back lost customers\n",
    "    - Supply-chain\n",
    "        - moving products and employees around\n",
    "- Bioinformatics / Diagnosis\n",
    "    - A -> B relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rules\n",
    "\n",
    "{ Beer } -> {Cigarettes} # how often do beer buyers by cigarettes\n",
    "{ Gas } -> { Chips } \n",
    "{ Coffee } -> { Donut }\n",
    "\n",
    "All of these are rules. Associates two items and makes a causial relationship.\n",
    "\n",
    "Different directions are different rules. All rules are unidirectional. \n",
    "\n",
    "### Computational Cost\n",
    "\n",
    "N transactions and d items.\n",
    "\n",
    "Example (Amazon):\n",
    "25 million transactions and 6 billion items. (Very sparse)\n",
    "A order only contains 5 or 6 items out of billions. Which means that most of the values in a transaction is zero. \n",
    "\n",
    "### Robustness\n",
    "\n",
    "**Spurious Patterns** (interpetation) \n",
    "Noninterpetible patterns. \n",
    "\n",
    "Example: \n",
    "Everybody that buys cavier buys slime. \n",
    "\n",
    "The patterns don't make any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction data representation\n",
    "\n",
    "Most of the time you get handed a list representation. You have to transfer to a matrix representation to do analysis. We have Binary variables/features for each inventory item.\n",
    "\n",
    "\n",
    "Presence is more important than absences. \n",
    "\n",
    "You lose a lot of information by switching to a matrix model.\n",
    "\n",
    "**Item sets** \n",
    "I = ${i_1 , i_2, ..., i_d} $\n",
    "\n",
    "**Transaction set**\n",
    "T = ${t_1, t_2, ......, t_n}$\n",
    "\n",
    "$t_j$ = ${i_4, i_3, i_7}$\n",
    "\n",
    "Any subset of i is an itemset\n",
    "\n",
    "Transactions are itemsets. if there are k items than it is a k_itemset. \n",
    "\n",
    "**width** is the number of items in a transaction.\n",
    "\n",
    "**Support Count:** \n",
    "itemset X, support count of X is #of transactions that contain X\n",
    "\n",
    "Example \n",
    "\n",
    "I have billions of transaction, I take an itemset {donuts, coffee} I find all transaction that have my itemset, that is the support for my itemset.\n",
    "\n",
    "**Support** = $$ \\frac{Support Count} {total transaction number} $$\n",
    "\n",
    "**Association Rule:** (Implication) \n",
    "\n",
    "X -> Y\n",
    "\n",
    "X and Y = {} disjoint itemsets. \n",
    "\n",
    "**Support of a rule:**\n",
    "s(x -> y) is the support count of (x union y) and divided by total transaction\n",
    "    - Take the two sets and combine them and calculate the support\n",
    "    \n",
    "**Normalize support count**\n",
    "c(x -> y) = support count of (x union y) divided by support count (x).\n",
    "\n",
    "A high ratio tells me that there is alot of confidence. \n",
    "\n",
    "**Interesting** if something doesn't have enough support it isn't interesting.\n",
    "\n",
    "**Reliable** is this implication occuring over and over again if not than it isn't good to look at. \n",
    "\n",
    "**Low Support** is a spurious association\n",
    "\n",
    "**Low Cofidence** is a conditional probability P(Y|X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Occurence & Causality\n",
    "\n",
    "If support count (X union Y) = support count (X union (X intersect Y)) = support count (X) + support count (Y) - support count (X intersect Y). \n",
    "\n",
    "Association rule is all about counting and probabilities based on the set of rules I am counting from. \n",
    "\n",
    "Association mining is easy when looking at single items it becomes hard when looking at sets of all kinds of items in your inventory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Discovery\n",
    "\n",
    "find all rules with support of a certain minsup(can be 50%) or more. and a confidence of more than a certain confidence of minconfs value.\n",
    "\n",
    "if I have a k-itemset of X union Y then K! # of possible rules.\n",
    "\n",
    "if the support of a union of X and Y is less than minsup than we **Prune** because this item set is insignificant.\n",
    "\n",
    "**Frequent Itemset Generation** find all itemset with S > minsup (Usually computational expensive)\n",
    "\n",
    "**Rule Generation** find all rules given frequent item set with c > minconf\n",
    "\n",
    "The conditional probability (**Confidence measurement**) is the one that determines which rules are significant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
